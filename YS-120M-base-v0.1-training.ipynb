{"cells":[{"cell_type":"markdown","metadata":{"id":"Q6YwsWszv6Pt"},"source":["# Why Snakes?\n","\n","This is the training notebook for YS-120M-base-v0.1.\n","\n","This code is MIT licensed. `Use it at your own risk`.\n","\n","There's a lot of garbage code in here as I iterated relatively fast to figure out what worked, or made small improvements. The best use of this project is to learn from it, but that's hampered by the junk. You are better off looking to the main branch for updates--which aren't checked in at the moment I'm writing this--but those changes for v0.2 will make the code more legible, even if still imperfect.\n","\n","My personal machine has an Intel 13th generation processor, which is among those impacted by the flaw that causes a random Blue Screen of Death. In an effort to push through, I brought training to Collab. It turns out, that collab likes to check if you are human at random intervals and then stop if you aren't watching at that moment. If you don't mind a fairly annoying experience, $50 to train a model on an L4 isn't outragous. It's more frustrating when you already own a better GPU and can't use it because you have a flawed CPU.\n","\n","### You are going to ask:\n","\n","* Why is it only 120 million parameters?\n"," - I needed it plus a chunk of training data to fit on an L4 (22.5 GB of gpu memory)\n"," - I inexplicably used longs for the token size rather than unsigned 16-bit ints.\n"," - After a lot of experimentation, I found that I needed a batch size of at least 64 to (mostly) train stably.\n"," - I didn't think of using a smaller sequence length with a bigger batch size to get closer to fully trained before switching to a longer sequence length and a small batch size until I had already spent a lot of time training.\n"," - I wanted to finish v0.1 before starting a new round of training with my learnings -- otherwise, I'd keep learning and you'd never see a v0.1\n","\n","* Why didn't you use a BPE-based tokenizer?\n"," - BPE is super well suited to the transformer architecture where you have a limited context size.\n"," - With Mamba 2, I have an unlimited context size. What's more a lot of the tokens that are fractions of a word don't have any inherent meaning.\n"," - BPE compresses text in a meaningful way, and most importantly, compresses out spaces, which would use up valuable tokens.\n"," - With Mamba 2, I have an unlimited context and can give the spaces meaning -- think of Python, multiple spaces mean something.\n"," - I can also tread things like capitalization, italics, underline, etc. as a token -- a modifier on the coming word. And that also has meaning without changing the base word directly, making it easier for the model to realize this is the same word but with another token influencing it -- just like a normal sentence.\n","\n","* Why didn't you use the HuggingFace library?\n"," - I appreciate what HuggingFace has done. They are among my favorite companies, but this project is about paving new roads.\n"," - I didn't want to battle learning the ins and outs of making a custom tokenizer and a custom model while learning to build the architecture I wanted.\n"," - Maybe some day I can contribute this to HuggingFace, but not until after I have the model I want.\n","\n","* Why didn't you use the Mamba 2 reference project?\n"," - It only runs on Linux with cuda. I wanted my project to work for people with a GPU or a CPU.\n","\n","* Does the world really need another small LLM?\n"," - Probably not, but I need to learn and if we as a community don't continue to work toward a functioning architure that isn't the Transformer architecture, only the largest of companies will be able to train models that are useful.\n"," - While I super appreciate the revolution that Transformers have given us, they've also created a moat that allows huge companies to hoard the knowledge, experience, and capability.\n"," - I want to know if I can do it--make a useful LLM.\n","\n","* If you want to make a useful LLM, why release v0.1 which you specifically say isn't useful?\n"," - Because I want to give really determined people a chance to learn from what I did. Even though it is really imperfect and not very useful, it's still slightly useful.\n"," - I will release a v0.2 which I expect to be better and easier to learn from, but these things take time and we're in a technology cycle that moves so fast that sitting on the knowledge helps nobody.\n","\n","* Why didn't you use more training data?\n"," - I decided to generate my own training data.\n"," - I like the idea of curating the data, and while I have only minimally curated it to this point, over time, I can continue to improve it.\n"," - Unfortunately, generating synthetic data takes time, and I wanted to release a v0.1 model as soon as I can. This is also why v0.2 will be trained on more data.\n"," - More data means slower training epochs, which means a longer time before I have a first version that I can evaluate.\n","\n","### Baseline categorical cross entropy loss\n","\n","Baseline categorical cross entropy loss let's us compare our loss against a model that has a perfectly equal chance of predicting any token.\n","\n","Baseline categorical cross entropy loss for our vocab = log(33,438) = 4.52\n","\n","We beat the baseline categorical cross entropy loss on our first training round, so at least the results aren't random.\n","\n","### Perplexity\n","\n","Perplexity helps us know that, for our vocabulary size, how well are we training. Categorical cross entropy is otherwise rather difficult to interpret. Having a good perpelexity isn't a great metric in terms of knowing how good your LLM is, but if you have a bad Perplexity, your model is bad.\n","\n","* Starting perplexity (e^loss) = e^5.2 = 181.27\n","* Final perplexity (e^loss) = e^? = ? [will fill in when i have the final resutls]\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":3710,"status":"ok","timestamp":1724243773520,"user":{"displayName":"Erik Hyrkas","userId":"03264676265568449270"},"user_tz":420},"id":"37qXhfo2wFX6"},"outputs":[],"source":["import pickle\n","import string\n","import time\n","import os\n","import torch\n","import re\n","import random\n","import math\n","\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","from torch.utils.data import TensorDataset, DataLoader, random_split, IterableDataset\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1724243773521,"user":{"displayName":"Erik Hyrkas","userId":"03264676265568449270"},"user_tz":420},"id":"rQok2Y2caLbT","outputId":"ae5cd7e5-81bd-4daa-d5d1-7a183c9d7b9d"},"outputs":[{"output_type":"stream","name":"stdout","text":["env: YS_LLM_BASE_PATH=/content/drive/MyDrive/ys-llm/\n"]}],"source":["%env YS_LLM_BASE_PATH=/content/drive/MyDrive/ys-llm/"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1724243773521,"user":{"displayName":"Erik Hyrkas","userId":"03264676265568449270"},"user_tz":420},"id":"WBw19OrqjtqX","outputId":"6c51d6fc-6048-4fa3-b117-6fae9b728e8c"},"outputs":[{"output_type":"stream","name":"stdout","text":["nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2023 NVIDIA Corporation\n","Built on Tue_Aug_15_22:02:13_PDT_2023\n","Cuda compilation tools, release 12.2, V12.2.140\n","Build cuda_12.2.r12.2/compiler.33191640_0\n"]}],"source":["!nvcc --version"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1724243773521,"user":{"displayName":"Erik Hyrkas","userId":"03264676265568449270"},"user_tz":420},"id":"uN2xWtF3jvSG"},"outputs":[],"source":["# from numba import cuda\n","# device = cuda.get_current_device()\n","# device.reset()"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1724243773521,"user":{"displayName":"Erik Hyrkas","userId":"03264676265568449270"},"user_tz":420},"id":"OlYJdneU83zT","outputId":"dc64e97c-5b37-424b-fc58-4b9b504f6de8"},"outputs":[{"output_type":"stream","name":"stdout","text":["2.3.1+cu121\n","True\n","12.1\n","True\n","1\n","0\n","NVIDIA L4\n","23802544128\n"]}],"source":["print(torch.__version__)\n","print(torch.cuda.is_available())\n","print(torch.version.cuda)\n","print(torch.cuda.is_available())\n","print(torch.cuda.device_count())\n","print(torch.cuda.current_device())\n","print(torch.cuda.get_device_name(torch.cuda.current_device()))\n","print(torch.cuda.get_device_properties(0).total_memory)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1724243773521,"user":{"displayName":"Erik Hyrkas","userId":"03264676265568449270"},"user_tz":420},"id":"KiMf382xj0bm"},"outputs":[],"source":["# %env CUDA_LAUNCH_BLOCKING=1"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1724243773521,"user":{"displayName":"Erik Hyrkas","userId":"03264676265568449270"},"user_tz":420},"id":"9PRu3Tnr18mk","outputId":"0f6f61c8-d8e5-4e9b-8ee6-ea2f34772e3e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":7}],"source":["torch.cuda.is_available()"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10027,"status":"ok","timestamp":1724243783545,"user":{"displayName":"Erik Hyrkas","userId":"03264676265568449270"},"user_tz":420},"id":"NDQixfmmjTav","outputId":"87fd1c91-3391-4377-cf7a-a7851f63b8fb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# /content/drive/MyDrive/ys-llm/training_data\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1724243783545,"user":{"displayName":"Erik Hyrkas","userId":"03264676265568449270"},"user_tz":420},"id":"f46gMR1XaN_E"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1724243783546,"user":{"displayName":"Erik Hyrkas","userId":"03264676265568449270"},"user_tz":420},"id":"pLHZuh_9wAO_"},"outputs":[],"source":["\n","DEBUG_TOKENIZER = False\n","\n","\n","class Tokenizer:\n","    def __init__(self):\n","        self.index_to_word = {}\n","        self.word_to_index = {}\n","        self.current_index = 0\n","        self.initialized = False\n","        self.special_tokens = [\"<upper>\", \"<shout>\", \"<start>\", \"<end>\", \"<space>\", \"<newline>\"]\n","\n","    def _add_to_vocab(self, word):\n","        if word not in self.word_to_index:\n","            self.index_to_word[self.current_index] = word\n","            self.word_to_index[word] = self.current_index\n","            self.current_index += 1\n","\n","    def _initialize_vocabulary(self):\n","        if not self.initialized:\n","            for token in self.special_tokens:\n","                self._add_to_vocab(token)\n","            self._add_to_vocab('_')\n","            # Note:\n","            # We have to keep the model small, and there's no intention\n","            # of being exclusionary, but I have to focus on the text that I know.\n","            # If you are taking and modifying this code, I highly recommend\n","            # streamlining this for the language you will train on and use.\n","            # I don't have the money to train a model that can handle all languages.\n","\n","            # Initialize single-letter printable tokens for basic ASCII and common Western European characters\n","            ascii_range = list(range(32, 127))  # Basic printable ASCII\n","            extended_chars = [\n","                'é', 'è', 'ê', 'ë', 'à', 'á', 'â', 'ä', 'å', 'ç', 'ì', 'í', 'î', 'ï',\n","                'ñ', 'ò', 'ó', 'ô', 'ö', 'ù', 'ú', 'û', 'ü', 'ý', 'ÿ', 'ø', 'å', 'Æ',\n","                'æ', 'œ', 'ß', 'ñ', '¿', '¡', '€', '£'\n","            ]\n","            extended_chars += [char.upper() for char in extended_chars if char.islower()]\n","\n","            # Add ASCII characters to vocabulary\n","            for i in ascii_range:\n","                char = chr(i)\n","                self._add_to_vocab(char)\n","\n","            # Add common Western European accented characters and symbols\n","            for char in extended_chars:\n","                self._add_to_vocab(char)\n","\n","            self.initialized = True\n","\n","    def learn_new_vocab(self, document: str):\n","        if not self.initialized:\n","            self._initialize_vocabulary()\n","        words = self.to_words(document)\n","        for word in words:\n","            if word not in self.word_to_index:\n","                self._add_to_vocab(word)\n","\n","    @staticmethod\n","    def _split_text(text):\n","        tokens = []\n","\n","        # Define the regular expressions for the tokens, ordered by priority\n","        space_regex = r'\\s+'  # One or more spaces\n","        tag_regex = r'<[a-z]+>'  # <lowercase letters>\n","        cap_word_regex = r'[A-Z][a-zà-ÿ]*'  # Capital followed by lowercase, including accented characters\n","        all_caps_regex = r'[A-Z][A-ZÀ-ß]+'  # All capital letters in a row, including accented uppercase characters\n","        lower_word_regex = r'[a-zà-ÿ]+'  # One or more lowercase letters, including accented characters\n","        single_cap_regex = r'[A-ZÀ-ß]'  # A single capital word (like 'A' or accented capital)\n","        number_regex = r'\\d+'  # One or more numbers\n","        symbol_regex = r'[^\\w\\s]|_'  # Single symbol or punctuation mark\n","\n","        # Create a combined regex to match all tokens\n","        combined_regex = f'({space_regex}|{tag_regex}|{cap_word_regex}|{all_caps_regex}|{lower_word_regex}|{single_cap_regex}|{number_regex}|{symbol_regex})'\n","\n","        # Find all matches using the combined regex\n","        matches = re.findall(combined_regex, text)\n","\n","        for match in matches:\n","            tokens.append(match)\n","\n","        return tokens\n","\n","    def to_words(self, text: str):\n","        tokens = self._split_text(text)\n","\n","        result = []\n","        for token in tokens:\n","            if not token:\n","                continue\n","            if DEBUG_TOKENIZER:\n","                print(f\"Processing token: {token}\")  # Debugging statement\n","            if '\\n' in token:\n","                result.append('<newline>')\n","            elif token.isspace():\n","                result.append('<space>')\n","            elif re.match(r'<[a-z]+>', token):\n","                result.append(token)  # tag\n","            elif re.match(r'[A-Z][a-zà-ÿ]*', token):\n","                result.append('<upper>')\n","                result.append(token.lower())\n","            elif re.match(r'[A-Z][A-ZÀ-ß]+', token):\n","                result.append('<shout>')\n","                result.append(token.lower())\n","            elif re.match(r'[a-zà-ÿ]+', token):\n","                result.append(token)\n","            elif re.match(r'[A-ZÀ-ß]', token):\n","                result.append(token.lower())\n","            else:\n","                result.append(token)\n","\n","        if DEBUG_TOKENIZER:\n","            print(f\"Final token list: {result}\")  # Debugging statement\n","        return result\n","\n","    def tokenize(self, text):\n","        if not self.initialized:\n","            self._initialize_vocabulary()\n","        tokens = []\n","        words = self.to_words(text)\n","        for word in words:\n","            if word in self.word_to_index:\n","                tokens.append(self.word_to_index[word])\n","            else:\n","                if DEBUG_TOKENIZER:\n","                    print(f\"Word '{word}' not found in vocabulary. Using individual characters.\")\n","                for letter in word:\n","                    if letter in self.word_to_index:\n","                        tokens.append(self.word_to_index[letter])\n","                    elif DEBUG_TOKENIZER:\n","                        print(f\"Character '{letter}' wasn't in vocabulary. Skipping!\")\n","        return tokens\n","\n","    def detokenize(self, tokens):\n","        text = ''\n","        capitalize_next = False\n","        shout_next = False\n","        for token in tokens:\n","            word = self.index_to_word[token]\n","            if word == '<space>':\n","                text += ' '\n","            elif word == '<newline>':\n","                text += '\\n'\n","            elif word == '<upper>':\n","                capitalize_next = True\n","            elif word == '<shout>':\n","                shout_next = True\n","            elif word == '_':\n","                text += '_'\n","            elif word in self.special_tokens:\n","                continue  # Skip rendering of standalone special tokens\n","            else:\n","                if capitalize_next:\n","                    word = word.capitalize()\n","                    capitalize_next = False\n","                if shout_next:\n","                    word = word.upper()\n","                    shout_next = False\n","                text += word\n","        return text\n","\n","    def vocab_size(self):\n","        return self.current_index\n","\n","    def save(self, filepath):\n","        with open(filepath, 'wb') as f:\n","            pickle.dump(\n","                (self.index_to_word, self.word_to_index, self.current_index, self.initialized), f)\n","\n","    def load(self, filepath):\n","        with open(filepath, 'rb') as f:\n","            self.index_to_word, self.word_to_index, self.current_index, self.initialized = pickle.load(f)\n","        if DEBUG_TOKENIZER:\n","            print(\"Tokenizer vocabulary size: \", self.vocab_size())\n","\n","    def print_vocabulary(self):\n","        print(self.word_to_index)\n","        print(\"Tokenizer vocabulary size: \", self.vocab_size())\n","\n","    def get_end_token(self):\n","        if not self.initialized:\n","            self._initialize_vocabulary()\n","        return self.word_to_index['<end>']\n","\n","    def get_start_token(self):\n","        if not self.initialized:\n","            self._initialize_vocabulary()\n","        return self.word_to_index['<start>']\n"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1724243783546,"user":{"displayName":"Erik Hyrkas","userId":"03264676265568449270"},"user_tz":420},"id":"hiSzo7uXukrI"},"outputs":[],"source":["ATTENTION_DEBUG = False\n","\n","\n","class Attention(nn.Module):\n","    def __init__(self, state_dim, input_dim, output_dim, block_size=32, dropout_rate=0.1):\n","        super(Attention, self).__init__()\n","        self.state_dim = state_dim\n","        self.block_size = block_size\n","        self.P = nn.Parameter(torch.eye(state_dim).repeat(state_dim, 1, 1) * 0.1 + 0.01)\n","        self.Q = nn.Parameter(torch.eye(state_dim, input_dim) * 0.1 + 0.01)\n","        self.R = nn.Parameter(torch.eye(output_dim, state_dim) * 0.1 + 0.01)\n","        self.S = nn.Parameter(torch.eye(output_dim, input_dim) * 0.1 + 0.01)\n","        self.layer_norm = nn.LayerNorm(state_dim)\n","        self.input_dropout = nn.Dropout(p=dropout_rate)\n","        # self.attn_dropout = nn.Dropout(p=dropout_rate)\n","        self.output_dropout = nn.Dropout(p=dropout_rate)\n","\n","    def forward(self, x):\n","        device = x.device\n","        batch_size, sequence_length, input_dim = x.shape\n","        outputs = []\n","        state = torch.zeros(batch_size, self.state_dim, device=device)\n","\n","        x = self.input_dropout(x)\n","\n","        p_expanded = self.P.unsqueeze(0)\n","        q_expanded = self.Q.expand(batch_size, -1, -1)\n","        s_expanded = self.S.expand(batch_size, -1, -1)\n","\n","        for start in range(0, sequence_length, self.block_size):\n","            end = min(start + self.block_size, sequence_length)\n","            x_block = x[:, start:end, :]\n","\n","            for t in range(x_block.shape[1]):\n","                input_t = x_block[:, t, :].unsqueeze(-1)\n","\n","                state_quad = torch.matmul(state.unsqueeze(-1), state.unsqueeze(-2))\n","                state = torch.einsum('bij,bijk->bk', state_quad, p_expanded) + \\\n","                        torch.matmul(q_expanded, input_t).squeeze(-1)\n","\n","                state = self.layer_norm(state)\n","\n","                # state = self.attn_dropout(state)\n","\n","                output_t = torch.matmul(self.R, state.unsqueeze(-1)).squeeze(-1) + \\\n","                           torch.matmul(s_expanded, input_t).squeeze(-1)\n","                outputs.append(output_t.unsqueeze(1))\n","\n","            if ATTENTION_DEBUG:\n","                if torch.isnan(state).any():\n","                    print(f\"NaN detected at step {start}-{end} in state.\")\n","                    raise ValueError(\"NaN detected in state after block processing\")\n","\n","                state_norm = torch.norm(state, p=2, dim=-1)\n","                if torch.any(state_norm > 1e5):\n","                    print(f\"Exploding state detected at step {start}-{end} with norm: {state_norm.max().item()}\")\n","                elif torch.any(state_norm < 1e-5):\n","                    print(f\"Vanishing state detected at step {start}-{end} with norm: {state_norm.min().item()}\")\n","\n","            state = state.detach()\n","\n","        final_output = torch.cat(outputs, dim=1)\n","\n","        final_output = self.output_dropout(final_output)\n","\n","        if ATTENTION_DEBUG:\n","            if torch.isnan(final_output).any():\n","                raise ValueError(\"NaN detected in final output\")\n","\n","            final_output_norm = torch.norm(final_output, p=2, dim=-1)\n","            if torch.any(final_output_norm > 1e5):\n","                print(f\"Exploding final output detected with norm: {final_output_norm.max().item()}\")\n","            elif torch.any(final_output_norm < 1e-5):\n","                print(f\"Vanishing final output detected with norm: {final_output_norm.min().item()}\")\n","\n","        return final_output"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1724243783546,"user":{"displayName":"Erik Hyrkas","userId":"03264676265568449270"},"user_tz":420},"id":"YJtcR2xHvfmm"},"outputs":[],"source":["class LanguageModel(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim=448, state_dim=448, output_dim=448):\n","        super(LanguageModel, self).__init__()\n","        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n","        self.state_space_model = Attention(state_dim=state_dim, input_dim=embedding_dim, output_dim=output_dim)\n","        self.layer_norm = nn.LayerNorm(output_dim)\n","        self.output_layer = nn.Linear(output_dim, vocab_size)\n","\n","    def forward(self, input_tokens):\n","        embedded = self.embedding(input_tokens)\n","        if len(embedded.shape) == 2:\n","            embedded = embedded.unsqueeze(0)\n","        context_representation = self.state_space_model(embedded)\n","        context_representation = self.layer_norm(context_representation)\n","        output = self.output_layer(context_representation)\n","        return output\n","\n","    def predict_next_token(self, input_tokens):\n","        output = self.forward(input_tokens)\n","        next_token_id = torch.argmax(output[:, -1, :], dim=-1).squeeze()\n","        return next_token_id.item()\n","\n","    def predict_next_token_softmax(self, input_tokens, top_p=0.9):\n","        output = self.forward(input_tokens)\n","\n","        logits = output[:, -1, :].squeeze()\n","        probs = torch.softmax(logits, dim=-1)\n","        sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n","        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n","        cutoff_index = torch.searchsorted(cumulative_probs, top_p)\n","        top_p_probs = sorted_probs[:cutoff_index + 1]\n","        top_p_indices = sorted_indices[:cutoff_index + 1]\n","        next_token_id = torch.multinomial(top_p_probs, 1)\n","\n","        return top_p_indices[next_token_id].item()"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1724243783546,"user":{"displayName":"Erik Hyrkas","userId":"03264676265568449270"},"user_tz":420},"id":"d1Qqj7FH9i7Z"},"outputs":[],"source":["\n","class ModelInterface:\n","    def __init__(self, model_save_path=\"model.bin\", tokenizer_save_path=\"tokenizer.pkl\"):\n","        self.tokenizer = Tokenizer()\n","        self.tokenizer.load(tokenizer_save_path)\n","        self.end_token = self.tokenizer.get_end_token()\n","        self.model = LanguageModel(vocab_size=self.tokenizer.vocab_size())\n","        self.model.load_state_dict(torch.load(model_save_path), strict=False)\n","        self.model.eval()\n","        self.device_name = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","        print(f\"Using {self.device_name}\")\n","        self.device = torch.device(self.device_name)\n","        self.model.to(self.device)\n","\n","    def count_parameters(self):\n","        total_params = sum(p.numel() for p in self.model.parameters())\n","        return total_params\n","\n","    def vocab_size(self):\n","        return self.tokenizer.vocab_size()\n","\n","    def complete(self, current_context: str, top_p: float = 0.9, max_tokens: int = 100000):\n","        tokens = self.tokenizer.tokenize(current_context)\n","        for i in range(max_tokens):\n","            input_tokens = torch.tensor(tokens).to(self.device)\n","            if top_p >= 1.0 or top_p <= 0.0:\n","                next_token = self.model.predict_next_token(input_tokens)\n","            else:\n","                next_token = self.model.predict_next_token_softmax(input_tokens, top_p)\n","            tokens.append(next_token)\n","            if next_token == self.end_token:\n","                break\n","        result = self.tokenizer.detokenize(tokens)\n","        return result\n","\n","    def prompt(self, prompt: str, top_p: float = 0.9, max_tokens: int = 100000):\n","        new_prompt = prompt\n","        if '<start>' not in prompt:\n","            new_prompt = prompt + '\\n<start>'\n","\n","        result = self.complete(new_prompt, top_p, max_tokens)\n","        return result.replace('<end>', '').strip()\n"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1724243783546,"user":{"displayName":"Erik Hyrkas","userId":"03264676265568449270"},"user_tz":420},"id":"yP2-cyuzz2LN"},"outputs":[],"source":["class TextDataset(IterableDataset):\n","    def __init__(self, files, tokenizer, context_length, shuffle_files=True):\n","        self.files = files\n","        self.tokenizer = tokenizer\n","        self.context_length = context_length\n","        self._length = None\n","        if shuffle_files:\n","            random.shuffle(self.files)\n","\n","    def __iter__(self):\n","        for file in self.files:\n","            with open(file, 'r', encoding='utf-8') as f:\n","                text = f.read()\n","                tokens = self.tokenizer.tokenize(text)\n","                for i in range(0, len(tokens) - self.context_length, self.context_length):\n","                    x = tokens[i:i + self.context_length]\n","                    y = tokens[i + 1:i + 1 + self.context_length]\n","                    yield torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n","\n","    def __len__(self):\n","        if self._length is None:\n","            # Calculate the length by iterating through the dataset\n","            count = 0\n","            for file in self.files:\n","                with open(file, 'r', encoding='utf-8') as f:\n","                    text = f.read()\n","                    tokens = self.tokenizer.tokenize(text)\n","                    count += len(tokens) // self.context_length\n","            self._length = count\n","        return self._length\n"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1724243783546,"user":{"displayName":"Erik Hyrkas","userId":"03264676265568449270"},"user_tz":420},"id":"u0z5PeYhzvzO"},"outputs":[],"source":["class EarlyStopping:\n","    def __init__(self, patience=5, min_delta=0.001):\n","        self.patience = patience\n","        self.min_delta = min_delta\n","        self.best_loss = np.inf\n","        self.wait = 0\n","        self.stop_training = False\n","\n","    def check(self, current_loss):\n","        if (self.best_loss - current_loss) > self.min_delta:\n","            self.best_loss = current_loss\n","            self.wait = 0\n","        else:\n","            self.wait += 1\n","            if self.wait >= self.patience:\n","                self.stop_training = True"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1724243783546,"user":{"displayName":"Erik Hyrkas","userId":"03264676265568449270"},"user_tz":420},"id":"rsXMf0OIwbyg"},"outputs":[],"source":["\n","def train_model(model, train_loader, val_loader, optimizer, criterion, scheduler, epochs=100, max_grad_norm=1.0,\n","                patience=5, accumulation_steps=4, no_validation=False):\n","    print(\"Training model...\")\n","    device_name = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    print(f\"Using {device_name}\")\n","    device = torch.device(device_name)\n","\n","    model.to(device)\n","    early_stopping = EarlyStopping(patience=patience)\n","\n","    if device_name == \"cuda\":\n","        # Initialize GradScaler for mixed precision if using GPU\n","        scaler = torch.cuda.amp.GradScaler()\n","    else:\n","        scaler = None  # No scaler needed for CPU\n","\n","    base_path = os.getenv(\"YS_LLM_BASE_PATH\", \"./\")\n","\n","    print(\"First epoch starting...\")\n","    parameters_shown = False\n","    best_loss = float('inf')\n","    for epoch in range(epochs):\n","        model.train()\n","        epoch_loss = 0.0\n","        optimizer.zero_grad()  # Initialize the optimizer at the start of each epoch\n","\n","        for i, (inputs, targets) in enumerate(train_loader):\n","            inputs, targets = inputs.to(device), targets.to(device)\n","\n","            if device_name == \"cuda\":\n","                # Use autocast for mixed precision on GPU\n","                with torch.cuda.amp.autocast():\n","                    outputs = model(inputs)\n","                    batch_size, sequence_length, vocab_size = outputs.shape\n","                    outputs = outputs.view(batch_size * sequence_length, vocab_size)\n","                    targets = targets.view(batch_size * sequence_length)\n","\n","                    loss = criterion(outputs, targets) / accumulation_steps\n","                # Scale the loss before backpropagation\n","                scaler.scale(loss).backward()\n","                # if not parameters_shown:\n","                #     total_params = sum(p.numel() for p in model.parameters())\n","                #     print(f\"Total number of parameters: {total_params}\")\n","                #     parameters_shown = True\n","            else:\n","                # Full precision for CPU\n","                outputs = model(inputs)\n","                batch_size, sequence_length, vocab_size = outputs.shape\n","                outputs = outputs.view(batch_size * sequence_length, vocab_size)\n","                targets = targets.view(batch_size * sequence_length)\n","\n","                loss = criterion(outputs, targets) / accumulation_steps\n","                loss.backward()\n","\n","            # Accumulate gradients and update model after a certain number of steps\n","            if (i + 1) % accumulation_steps == 0:\n","                if device_name == \"cuda\":\n","                    scaler.unscale_(optimizer)\n","                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n","                optimizer.step()\n","                if device_name == \"cuda\":\n","                    scaler.update()\n","                optimizer.zero_grad()  # Reset gradients for the next accumulation cycle\n","\n","            epoch_loss += loss.item() * accumulation_steps  # Multiply to undo the earlier division\n","\n","        if not no_validation:\n","            model.eval()\n","            val_loss = 0.0\n","            with torch.no_grad():\n","                for inputs, targets in val_loader:\n","                    inputs, targets = inputs.to(device), targets.to(device)\n","                    if device_name == \"cuda\":\n","                        with torch.cuda.amp.autocast():  # Use autocast for validation as well on GPU\n","                            outputs = model(inputs)\n","                            outputs = outputs.view(-1, outputs.size(-1))\n","                            targets = targets.view(-1)\n","                            loss = criterion(outputs, targets)\n","                    else:\n","                        outputs = model(inputs)\n","                        outputs = outputs.view(-1, outputs.size(-1))\n","                        targets = targets.view(-1)\n","                        loss = criterion(outputs, targets)\n","\n","                    val_loss += loss.item()\n","\n","            val_loss /= len(val_loader)\n","            print(f'Epoch {epoch + 1}, Loss: {epoch_loss / len(train_loader)}, Val Loss: {val_loss}')\n","            scheduler.step()\n","\n","            if math.isnan(val_loss) or math.isnan(epoch_loss) or math.isinf(val_loss) or math.isinf(epoch_loss):\n","                print(\"Stopping due to numerical instability.\")\n","                return False\n","\n","            if epoch_loss < best_loss:\n","                best_loss = epoch_loss\n","                torch.save(model.state_dict(), f\"{base_path}model_checkpoint.bin\")\n","                print(f\"Checkpoint saved at epoch {epoch + 1} to {base_path}model_checkpoint.bin\")\n","                if not parameters_shown:\n","                    total_params = sum(p.numel() for p in model.parameters())\n","                    print(f\"Total number of parameters: {total_params}\")\n","                    parameters_shown = True\n","\n","            early_stopping.check(val_loss)\n","            if early_stopping.stop_training:\n","                print(f\"Early stopping triggered at epoch {epoch + 1}\")\n","                break\n","        else:\n","            # Monitor training loss for stopping if no validation\n","            print(f'Epoch {epoch + 1}, Loss: {epoch_loss / len(train_loader)}')\n","            scheduler.step()\n","\n","            if math.isnan(epoch_loss) or math.isinf(epoch_loss):\n","                print(\"Stopping due to numerical instability.\")\n","                return False\n","\n","            if epoch_loss < best_loss:\n","                best_loss = epoch_loss\n","                torch.save(model.state_dict(), f\"{base_path}model_checkpoint.bin\")\n","                print(f\"New best model saved at epoch {epoch + 1}\")\n","                if not parameters_shown:\n","                    total_params = sum(p.numel() for p in model.parameters())\n","                    print(f\"Total number of parameters: {total_params}\")\n","                    parameters_shown = True\n","\n","            early_stopping.check(epoch_loss)\n","            if early_stopping.stop_training:\n","                print(f\"Early stopping triggered at epoch {epoch + 1}\")\n","                break\n","\n","    return True\n","\n","\n","def safe_tensor_conversion(data_list, dtype=torch.long):\n","    try:\n","        tensor = torch.tensor(data_list, dtype=dtype)\n","        return tensor\n","    except Exception as e:\n","        print(f\"Error during tensor conversion: {e}\")\n","        previous_len = None\n","        for i, data in enumerate(data_list):\n","            data_len = len(data)\n","            if previous_len is None:\n","                print(f\"Data length at index {i}: {data_len}\")\n","                previous_len = data_len\n","            if data_len != previous_len:\n","                print(f\"Data length at index {i}: {data_len}\")\n","                print(f\"Data at index {i}: {data}\")\n","            for token in data:\n","                if token is not None:\n","                    print(f\"None found in data at index {i}: {data}\")\n","        raise e\n","\n","\n","def prepare_training_data(texts, tokenizer, sequence_length=5):\n","    x_train_list, y_train_list = [], []\n","    for text in texts:\n","        tokens = tokenizer.tokenize(text)\n","        if len(tokens) < sequence_length + 1:\n","            continue  # Skip sequences that are too short\n","        for i in range(len(tokens) - sequence_length):\n","            intput_sequence = tokens[i:i + sequence_length]\n","            target_sequence = tokens[i + 1:i + sequence_length + 1]\n","            if len(intput_sequence) == sequence_length and len(target_sequence) == sequence_length:\n","                x_train_list.append(intput_sequence)  # Input sequence\n","                y_train_list.append(target_sequence)  # Target sequence\n","            else:\n","                print(\n","                    f\"Skipping a sequence with incorrect length: x_seq={len(intput_sequence)}, y_seq={len(target_sequence)}\")\n","\n","    x_train = safe_tensor_conversion(x_train_list)\n","    y_train = safe_tensor_conversion(y_train_list)\n","\n","    # print(f\"x_train shape: {x_train.shape}\")  # Expect (num_sequences, sequence_length)\n","    # print(f\"y_train shape: {y_train.shape}\")  # Expect (num_sequences, sequence_length)\n","\n","    return x_train, y_train\n","\n","\n","def split_dataset(dataset, val_split=0.2):\n","    val_size = int(len(dataset) * val_split)\n","    train_size = len(dataset) - val_size\n","    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","    return train_dataset, val_dataset\n","\n","\n","def do_train(training_sequence_length=5, batch_size=64, max_epochs=100, patience=5, model_save_path=\"model.bin\",\n","             tokenizer_save_path=\"tokenizer.pkl\", no_validation=False):\n","    tokenizer, train_loader, val_loader, number_of_samples = build_tokenizer_and_load_tokens(training_sequence_length,\n","                                                                                             batch_size,\n","                                                                                             tokenizer_save_path)\n","\n","    vocab_size = tokenizer.vocab_size()\n","    print(f\"Vocabulary size: {vocab_size}\")\n","\n","    model = LanguageModel(tokenizer.vocab_size())\n","\n","    base_path = os.getenv(\"YS_LLM_BASE_PATH\", \"./\")\n","    if os.path.exists(f\"{base_path}model_checkpoint.bin\"):\n","        model.load_state_dict(torch.load(f\"{base_path}model_checkpoint.bin\"))\n","        print(f\"Resumed training from {base_path}model_checkpoint.bin\")\n","        total_params = sum(p.numel() for p in model.parameters())\n","        print(f\"Number of parameters: {total_params}\")\n","\n","    total_training_steps = (number_of_samples // batch_size) * max_epochs\n","    if number_of_samples % batch_size != 0:\n","        # Add one step for each epoch to cover the last incomplete batch\n","        total_training_steps += max_epochs\n","\n","    optimizer = optim.AdamW(model.parameters(), lr=0.0005, weight_decay=1e-5)\n","    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.0005, total_steps=total_training_steps)\n","    criterion = nn.CrossEntropyLoss()\n","\n","    model_trained = train_model(model, train_loader, val_loader, optimizer, criterion, scheduler, epochs=max_epochs,\n","                                patience=patience, no_validation=no_validation)\n","    if model_trained:\n","        print(\"Saving model...\")\n","        torch.save(model.state_dict(), model_save_path)\n","        print(f\"Model saved to {model_save_path}\")\n","        total_params = sum(p.numel() for p in model.parameters())\n","        print(f\"Number of parameters: {total_params}\")\n","        print(f\"Vocabulary size: {vocab_size}\")\n","\n","\n","def build_tokenizer_and_load_tokens(training_sequence_length, batch_size, tokenizer_save_path, no_validation=False):\n","    print(\"Loading and tokenizing training data...\")\n","    training_file_names = get_training_file_names()\n","    # Split file names for training and validation\n","    random.shuffle(training_file_names)\n","    if no_validation:\n","        train_files = training_file_names\n","        val_files = []\n","    else:\n","        split_index = int(0.8 * len(training_file_names))\n","        train_files = training_file_names[:split_index]\n","        val_files = training_file_names[split_index:]\n","\n","    # Initialize the tokenizer\n","    tokenizer = Tokenizer()\n","    if os.path.exists(tokenizer_save_path):\n","        tokenizer.load(tokenizer_save_path)\n","        print(\"Loaded existing tokenizer.\")\n","    else:\n","        # Update tokenizer vocab by iterating over each document\n","        for file_path in training_file_names:\n","            with open(file_path, 'r', encoding='utf-8') as file:\n","                document = file.read()\n","                tokenizer.learn_new_vocab(document)  # Add document content to vocab\n","                del document  # Free memory as soon as possible\n","\n","        # Save the tokenizer with the built vocabulary\n","        tokenizer.save(tokenizer_save_path)\n","\n","    # DataLoader for batching (shuffle is done within the TextDataset)\n","    train_dataset = TextDataset(train_files, tokenizer, training_sequence_length)\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n","\n","    if no_validation:\n","        val_loader = None\n","    else:\n","        val_dataset = TextDataset(val_files, tokenizer, training_sequence_length)\n","        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","\n","    number_of_samples = len(train_dataset)\n","\n","    return tokenizer, train_loader, val_loader, number_of_samples\n","\n","\n","def get_training_file_names(directory=\"training_data\"):\n","    base_path = os.getenv(\"YS_LLM_BASE_PATH\", \"./\")\n","    file_names = []\n","    for filename in os.listdir(f\"{base_path}{directory}\"):\n","        if filename.endswith(\".txt\") or filename.endswith(\".md\"):\n","            filepath = os.path.join(f\"{base_path}{directory}\", filename)\n","            file_names.append(filepath)\n","    return file_names\n","\n","\n","def notebook_do_train(no_validation=False):\n","    base_path = os.getenv(\"YS_LLM_BASE_PATH\", \"./\")\n","    model_path = f\"{base_path}model.bin\"\n","    tokenizer_path = f\"{base_path}tokenizer.pkl\"\n","    do_train(training_sequence_length=96, batch_size=20,\n","             max_epochs=400, patience=10,\n","             model_save_path=model_path,\n","             tokenizer_save_path=tokenizer_path,\n","             no_validation=no_validation)\n"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":799,"status":"ok","timestamp":1724243901992,"user":{"displayName":"Erik Hyrkas","userId":"03264676265568449270"},"user_tz":420},"id":"Vcv4tryBwuUs"},"outputs":[],"source":["def interactive_interface():\n","    print(\"This is a usage example. This LLM will completely make up things and be wrong. Don't follow its advice and \"\n","          \"don't expect good results. There is no warranty. Use at your own risk.\")\n","    base_path = os.getenv(\"YS_LLM_BASE_PATH\", \"./\")\n","    model_path = f\"{base_path}model.bin\"\n","    tokenizer_path = f\"{base_path}tokenizer.pkl\"\n","    print(\"Loading...\")\n","    interface = ModelInterface(model_save_path=model_path, tokenizer_save_path=tokenizer_path)\n","    params = interface.count_parameters()\n","    print(f\"Number of parameters: {params}\")\n","    print(f\"Vocabulary size: {interface.vocab_size()}\")\n","    print(\"Type a prompt that the model will write a story about. Type 'exit' to exit.\")\n","    while True:\n","        next_input = input(\"> \")\n","        if next_input == 'exit':\n","            break\n","        result = interface.complete(next_input.strip(), top_p=1.0, max_tokens=96)\n","        print(f\"\\n{result}\")"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"DfpY9QIDwf9O","executionInfo":{"status":"ok","timestamp":1724243783546,"user_tz":420,"elapsed":5,"user":{"displayName":"Erik Hyrkas","userId":"03264676265568449270"}}},"outputs":[],"source":["# Uncomment to train\n","# notebook_do_train(no_validation=True)"]},{"cell_type":"markdown","metadata":{"id":"cvJDOjABlTla"},"source":["# Training runs for v0.1: 120M Parameters/33,438 token Vocab\n","\n","Starting with a sequence length of 72 was a mistake. I knew that it wasn't efficient, but I was trying to maximize memory usage. I should have stuck to a multiple of the attention block size and just increased the batch size. That would have been more stable and made future training easier. It was numerically unstable 2 of those first 3 runs with a sequence length of 72.\n","\n","There were three cases of numeric instability through the rest of the training, but that's one of the challenges with working with Mamba 2. It feels like you are always walking the edge. Most of the time, a training run was interrupted because colab stopped after some random time. This was frustrating, especially since I paid for the usage and paid extra for epochs that weren't allowed to finish. I speculate they charged me for at least 12 hours of time where they killed the run partway through an epoch before a save. I had to get up in the middle of the night to restart it. Not an experience I recommend and I won't be using colab after my compute units are gone. I read and re-read the colab rules and don't see how this breaks them -- but I suspect they are treating my account as free tier, despite paying for compute units.\n","\n","| length | batch | epochs | loss start | loss end |\n","|---|---|---|---|---|\n","| 72 | 64 | 9 | ~5.2 | ~3.1 |\n","| 72 | 64 | 21 | ~3.1 | ~2.4 |\n","| 72 | 64 | 14? | ~2.37 | ~2.0 |\n","| 32 | 256 | 11 | ~2.25 | ~2.0 |\n","| 32 | 256 | 17 | ~2.05 | ~1.7 |\n","| 32 | 256 | 9 | ~1.88 | ~1.6 |\n","| 32 | 256 | 35 | ~1.64 | ~1.09 |\n","| 32 | 256 | 9 | ~1.35 | ~1.04 |\n","| 32 | 256 | 34 | ~1.22 | ~0.75 |\n","| 64 | 80 | 4 | ~1.43 | ~1.07 |\n","| 64 | 80 | 24 | ~1.16 | ~0.6 |\n","| 96 | 20 | 3 | ~1.21 | ~1.02 |\n","| 96 | 20 | 7 | ~1.14 | ~0.75 |\n","| 96 | 20 | 7 | ~0.94 | ~0.64 |\n","| 96 | 20 | 5 | ~0.82 | ~0.61 |\n","| 96 | 20 | 1 | ~0.69 | - |\n","| 96 | 20 | 3 | ~0.69 | ~0.59 |\n","| 96 | 20 | 3 | ~0.65 | ~0.56 |\n","\n","## Learnings for v0.2\n","\n","For v0.2, I plan on starting off with a sequence length equal to the attention block size, and the biggest batch I can manage. It helps with numeric stability (not getting 0 or infinity as a weight.) Then after I getting closer to trained, switching to longer and longer sequence lengths (multiples of the attention block size) to ensure it has used multiple blocks during training."]},{"cell_type":"code","execution_count":21,"metadata":{"id":"-J2aKc1qwwHz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724243987803,"user_tz":420,"elapsed":18674,"user":{"displayName":"Erik Hyrkas","userId":"03264676265568449270"}},"outputId":"448d0390-a69f-42c3-b420-c9903c03766c"},"outputs":[{"name":"stdout","output_type":"stream","text":["This is a usage example. This LLM will completely make up things and be wrong. Don't follow its advice and don't expect good results. There is no warranty. Use at your own risk.\n","Loading...\n","Using cuda\n","Number of parameters: 120513182\n","Vocabulary size: 33438\n","Type a prompt that the model will write a story about. Type 'exit' to exit.\n","> write a story\n","\n","write a story about a 65-year-old geologist named Emily who embarks on a solo expedition into the American Southwest, only to discover a massive underground complex that holds secrets from her family's past, and must navigate a treacherous web of family loyalty and precious valuable \n","> exit\n"]}],"source":["# Uncomment to test the model\n","interactive_interface()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","provenance":[],"mount_file_id":"1RTBcAk2RvKb8V8erFKROq9JF7axpTYj9","authorship_tag":"ABX9TyOMpkIzbLj6ytIYwK4kURvO"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}